# ðŸ—ºï¸ MoodSwarm â€” Step-by-Step Learning Guide

> **Goal:** Understand the entire MoodSwarm project from scratch, one layer at a time.
> Each module builds on the previous one. Don't skip ahead.

---

## How to Use This Guide

1. **Read the listed files** in order â€” use the code links to jump to each file
2. **Answer the self-check questions** before moving to the next module
3. **Run the suggested commands** to see things in action
4. Estimated time per module: **30â€“60 minutes**

---

## Module 1: The Big Picture

> **Goal:** Understand *what* this project does and *why* it's structured this way.

### What is MoodSwarm?

An AI system that **mimics a specific person's writing style**. It does this by:

1. **Crawling** that person's online content (Medium articles, GitHub repos)
2. **Cleaning** & **chunking** the text into searchable pieces
3. **Embedding** those chunks into vectors (numbers that capture meaning)
4. **Retrieving** relevant chunks when you ask a question (RAG)
5. *(Future)* Fine-tuning an LLM to respond in that person's voice

### The Architecture Pattern: FTI

```
Feature Pipeline â†’ Training Pipeline â†’ Inference Pipeline
     (Weeks 1-3)       (Week 5-6)          (Week 7-8)
```

You've built Weeks 1â€“4 so far. That covers the **Feature Pipeline** + **RAG Retrieval**.

### Key Technologies

| What | Why |
|------|-----|
| **MongoDB** | Stores raw crawled documents (articles, repos) |
| **Qdrant** | Stores vector embeddings for semantic search |
| **ZenML** | Orchestrates multi-step pipelines |
| **Docker Compose** | Runs MongoDB + Qdrant locally |
| **Poetry** | Manages Python dependencies |

### Read These First
- [README.md](file:///Users/arkaj/Desktop/moodSwarm/README.md) â€” top-level overview (lines 1â€“80)
- [docker-compose.yml](file:///Users/arkaj/Desktop/moodSwarm/docker-compose.yml) â€” see what services run

### âœ… Self-Check
- [ ] Can you explain the FTI architecture in one sentence?
- [ ] What are the two databases and what does each store?
- [ ] What is ZenML's role?

---

## Module 2: Configuration & Infrastructure

> **Goal:** Understand how the project configures itself and connects to databases.

### Step 2.1 â€” Environment Settings

ðŸ“„ **Read:** [settings.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/settings.py)

This file uses **Pydantic Settings** to load `.env` variables. Key concepts:

- `Settings` class reads environment variables automatically
- Database hosts, API keys, model names â€” all configured here
- `@model_validator` handles defaults and validation

> **Concept: 12-Factor App** â€” Config lives in environment variables, not code.

### Step 2.2 â€” Database Connectors (Singleton Pattern)

ðŸ“„ **Read in order:**
1. [mongo.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/infrastructure/db/mongo.py) â€” MongoDB connector
2. [qdrant.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/infrastructure/db/qdrant.py) â€” Qdrant connector

Both use the **Singleton Pattern** â€” only ONE database connection is ever created, no matter how many times the class is instantiated.

```python
# The pattern:
class MongoDatabaseConnector:
    _instance = None
    def __new__(cls):
        if cls._instance is None:
            cls._instance = MongoClient(...)
        return cls._instance
```

> **Why Singleton?** Creating a new DB connection is expensive. Reusing one connection is fast.

### Step 2.3 â€” Docker Infrastructure

ðŸ“„ **Read:** [docker-compose.yml](file:///Users/arkaj/Desktop/moodSwarm/docker-compose.yml)

```
MongoDB  â†’ port 27017 (raw document store)
Qdrant   â†’ port 6333 (vector search engine)
```

### ðŸƒ Try It

```bash
# Start databases
docker compose up -d

# Verify they're running
docker compose ps
```

### âœ… Self-Check
- [ ] What does `__new__` do vs `__init__`? Why is it used for Singleton?
- [ ] Where does the database host URL come from?
- [ ] What happens if you call `MongoDatabaseConnector()` twice?

---

## Module 3: Domain Models (DDD Layer)

> **Goal:** Understand the data structures â€” how data is represented at every stage.

This is the most important module. The project uses **Domain-Driven Design (DDD)** â€” the domain models are pure Python classes with no external dependencies.

### Step 3.1 â€” The Two Base Classes

ðŸ“„ **Read in order:**
1. [nosql.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/domain/base/nosql.py) â€” MongoDB ODM
2. [vector.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/domain/base/vector.py) â€” Qdrant ODM

These are **custom ORMs** (Object-Relational Mappers):

| Base Class | Maps to | Key Methods |
|---|---|---|
| `NoSQLBaseDocument` | MongoDB | `save()`, `find()`, `to_mongo()`, `from_mongo()` |
| `VectorBaseDocument` | Qdrant | `bulk_insert()`, `search()`, `to_point()`, `from_record()` |

**Critical concept â€” ID transformation:**

```
Python:  id = UUID('a1b2c3...')     â† Python UUID object
MongoDB: _id = 'a1b2c3...'         â† renamed + stringified
Back:    id = UUID('a1b2c3...')     â† Pydantic coerces back
```

### Step 3.2 â€” Raw Documents (Week 2 output)

ðŸ“„ **Read:** [documents.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/domain/documents.py)

These represent **crawled data** stored in MongoDB:

```
UserDocument      â†’ users collection
ArticleDocument   â†’ articles collection
RepositoryDocument â†’ repositories collection
PostDocument      â†’ posts collection
```

Each has `content` (dict), `platform`, `author_id`, and `link`.

### Step 3.3 â€” Cleaned Documents (Week 3, Stage 1)

ðŸ“„ **Read:** [cleaned_documents.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/domain/cleaned_documents.py)

Cleaned versions stored in **Qdrant** (payload-only, no vectors yet). `content` is now a plain string instead of a dict.

### Step 3.4 â€” Chunks (Week 3, Stage 2)

ðŸ“„ **Read:** [chunks.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/domain/chunks.py)

Chunks split from cleaned documents. Key fields:
- `content` â€” the chunk text
- `chunk_id` â€” position index
- `document_id` â€” links back to parent document
- `id` â€” **MD5 hash of content** (deterministic = deduplication!)

### Step 3.5 â€” Embedded Chunks (Week 3, Stage 3)

ðŸ“„ **Read:** [embedded_chunks.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/domain/embedded_chunks.py)

Same as chunks but with `embedding: list[float]` â€” a 384-dimensional vector from `all-MiniLM-L6-v2`.

### Step 3.6 â€” Queries (Week 4)

ðŸ“„ **Read:** [queries.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/domain/queries.py)

Query objects follow the same pipeline as chunks: `Query` â†’ `EmbeddedQuery` (with vector). This ensures queries live in the **same vector space** as the content.

### Step 3.7 â€” Types & Exceptions

ðŸ“„ **Read:**
- [types.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/domain/types.py) â€” `DataCategory` enum
- [exceptions.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/domain/exceptions.py)

### ðŸ“Š The Data Transformation Chain

```
URL â†’ ArticleDocument â†’ CleanedArticleDocument â†’ ArticleChunk â†’ EmbeddedArticleChunk
       (MongoDB)          (Qdrant, no vector)     (split text)    (Qdrant, 384-dim)
```

### âœ… Self-Check
- [ ] What's the difference between `NoSQLBaseDocument` and `VectorBaseDocument`?
- [ ] Why are chunk IDs generated from MD5 hashes?
- [ ] How does `to_point()` transform a Python object for Qdrant?
- [ ] What does a `Query` object look like? How is it embedded?

---

## Module 4: ETL Pipeline â€” Crawling Data (Week 2)

> **Goal:** Understand how data gets from the internet into MongoDB.

### Step 4.1 â€” The Crawler Architecture

ðŸ“„ **Read in order:**
1. [dispatcher.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/crawlers/dispatcher.py) â€” URL routing
2. [base.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/crawlers/base.py) â€” BaseCrawler ABC

**Pattern: Strategy + Factory**

`CrawlerDispatcher` matches a URL against regex patterns and returns the right crawler:

```
medium.com/*   â†’ MediumCrawler
github.com/*   â†’ GithubCrawler
anything else  â†’ CustomArticleCrawler
```

### Step 4.2 â€” Individual Crawlers

ðŸ“„ **Read:**
- [medium.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/crawlers/medium.py) â€” Selenium headless browser
- [github.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/crawlers/github.py) â€” git clone + file walk
- [custom_article.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/crawlers/custom_article.py) â€” LangChain AsyncHtmlLoader

### Step 4.3 â€” Pipeline & Steps

ðŸ“„ **Read in order:**
1. [digital_data_etl.py](file:///Users/arkaj/Desktop/moodSwarm/pipelines/digital_data_etl.py) â€” pipeline definition
2. [get_or_create_user.py](file:///Users/arkaj/Desktop/moodSwarm/steps/etl/get_or_create_user.py) â€” Step 1
3. [crawl_links.py](file:///Users/arkaj/Desktop/moodSwarm/steps/etl/crawl_links.py) â€” Step 2

### Step 4.4 â€” Config

ðŸ“„ **Read:** [digital_data_etl.yaml](file:///Users/arkaj/Desktop/moodSwarm/configs/digital_data_etl.yaml)

### ðŸƒ Try It

```bash
# See the pipeline run (read-only, don't actually crawl unless intended)
python -m tools --run-etl --help
```

### âœ… Self-Check
- [ ] How does `CrawlerDispatcher` decide which crawler to use?
- [ ] What deduplication strategy prevents duplicate documents?
- [ ] What retry/resilience pattern does the crawl step use?
- [ ] Trace a Medium URL from config â†’ MongoDB document

---

## Module 5: Feature Engineering â€” Clean â†’ Chunk â†’ Embed (Week 3)

> **Goal:** Understand how raw text becomes searchable vectors.

### Step 5.1 â€” The Three Dispatchers

ðŸ“„ **Read:** [dispatchers.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/preprocessing/dispatchers.py)

Three dispatchers route documents to the right handler by `DataCategory`:

```
CleaningDispatcher  â†’ picks cleaning handler
ChunkingDispatcher  â†’ picks chunking handler
EmbeddingDispatcher â†’ picks embedding handler
```

### Step 5.2 â€” Cleaning Handlers

ðŸ“„ **Read:** [cleaning_data_handlers.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/preprocessing/cleaning_data_handlers.py)

Each handler converts raw `dict` content â†’ plain text string using regex operations.

ðŸ“„ **Read:** The cleaning operations in [operations/](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/preprocessing/operations/)

### Step 5.3 â€” Chunking Handlers (Two-Stage)

ðŸ“„ **Read:** [chunking_data_handlers.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/preprocessing/chunking_data_handlers.py)

**Two-stage chunking is critical:**

```
Stage 1: Split by content structure (paragraphs, sentences)
Stage 2: Cap at embedding model's max token length (256 tokens)
```

| Content | Stage 1 | Stage 2 |
|---------|---------|---------|
| Posts | 250 tokens / 25 overlap | Token cap at 256 |
| Articles | 1000-2000 chars, sentence-aware | Token cap at 256 |
| Repos | 1500 tokens / 100 overlap | Token cap at 256 |

### Step 5.4 â€” Embedding Handlers

ðŸ“„ **Read:** [embedding_data_handlers.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/preprocessing/embedding_data_handlers.py)

ðŸ“„ **Read:** [embeddings.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/networks/embeddings.py) â€” `EmbeddingModelSingleton`

The embedding model (`all-MiniLM-L6-v2`) converts text â†’ 384-dimensional float vectors.

### Step 5.5 â€” Pipeline & Steps

ðŸ“„ **Read in order:**
1. [feature_engineering.py](file:///Users/arkaj/Desktop/moodSwarm/pipelines/feature_engineering.py) â€” pipeline
2. [query_data_warehouse.py](file:///Users/arkaj/Desktop/moodSwarm/steps/feature_engineering/query_data_warehouse.py)
3. [clean.py](file:///Users/arkaj/Desktop/moodSwarm/steps/feature_engineering/clean.py)
4. [rag.py](file:///Users/arkaj/Desktop/moodSwarm/steps/feature_engineering/rag.py) â€” chunk + embed step
5. [load_to_vector_db.py](file:///Users/arkaj/Desktop/moodSwarm/steps/feature_engineering/load_to_vector_db.py)

### ðŸƒ Try It

```bash
# Inspect what's in Qdrant
python -m tools.qdrant_inspect list-collections
python -m tools.qdrant_inspect stats embedded_articles

# Run chunk analysis
python -m tools.chunk_analysis
```

### âœ… Self-Check
- [ ] Why is two-stage chunking needed? What problem does Stage 2 solve?
- [ ] What happens in `EmbeddingModelSingleton.__call__`?
- [ ] What design patterns are used? (Factory, Strategy, Dispatcher, Singleton)
- [ ] Trace a `CleanedArticleDocument` â†’ `ArticleChunk` â†’ `EmbeddedArticleChunk`

---

## Module 6: RAG Retrieval Pipeline (Week 4)

> **Goal:** Understand the full Retrieval-Augmented Generation pipeline.

This is the most complex module. Read slowly.

### Step 6.1 â€” Base Classes

ðŸ“„ **Read:** [base.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/rag/base.py)

Two ABCs:
- `PromptTemplateFactory` â€” produces LangChain prompt templates
- `RAGStep` â€” base for all RAG stages, supports `mock=True` for testing without API calls

### Step 6.2 â€” Prompt Templates

ðŸ“„ **Read:** [prompt_templates.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/rag/prompt_templates.py)

Two template factories:
- `QueryExpansionTemplate` â€” prompt that asks the LLM to generate N alternative queries
- `SelfQueryTemplate` â€” few-shot prompt to extract author name from a natural language query

### Step 6.3 â€” Self-Query (Pre-Retrieval Optimization #1)

ðŸ“„ **Read:** [self_query.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/rag/self_query.py)

**What it does:** Extracts the author name from a query â†’ looks up their UUID in MongoDB â†’ attaches `author_id` to the Query object for **filtered vector search**.

```
"What did Paul write about LLMs?"
  â†’ SelfQuery extracts "Paul Iusztin"
  â†’ Looks up UserDocument â†’ gets UUID
  â†’ Query.author_id = UUID('abc...')
  â†’ Qdrant search filtered by author_id
```

### Step 6.4 â€” Query Expansion (Pre-Retrieval Optimization #2)

ðŸ“„ **Read:** [query_expansion.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/rag/query_expansion.py)

**What it does:** Takes one query â†’ asks OpenAI to generate N diverse reformulations â†’ searches with all of them for better recall.

```
"How to fine-tune LLMs?"
  â†’ Expanded to 3 queries:
     1. "What techniques exist for LLM fine-tuning?"
     2. "How to adapt a pretrained language model?"
     3. "Fine-tuning methods like LoRA, QLoRA for LLMs"
```

### Step 6.5 â€” Reranking (Post-Retrieval Optimization)

ðŸ“„ **Read:** [reranking.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/rag/reranking.py)

ðŸ“„ **Also read:** the `CrossEncoderModelSingleton` in [embeddings.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/networks/embeddings.py)

**What it does:** After retrieving chunks, scores each `(query, chunk)` pair with a cross-encoder â†’ re-sorts by relevance â†’ returns top-K.

> **Why rerank?** Bi-encoder search (embedding similarity) is fast but approximate. Cross-encoder scoring is more accurate but slower. Reranking combines both.

### Step 6.6 â€” The Orchestrator: ContextRetriever

ðŸ“„ **Read:** [retriever.py](file:///Users/arkaj/Desktop/moodSwarm/llm_engineering/application/rag/retriever.py)

This is the **single most important file** in Week 4. It chains everything:

```mermaid
graph TD
    Q["User Query"] --> SQ["1. SelfQuery<br/>Extract author"]
    SQ --> QE["2. QueryExpansion<br/>Generate N variants"]
    QE --> PAR["3. Parallel Search<br/>ThreadPoolExecutor"]
    PAR --> S1["Search embedded_posts"]
    PAR --> S2["Search embedded_articles"]
    PAR --> S3["Search embedded_repos"]
    S1 & S2 & S3 --> DD["4. Deduplicate<br/>by UUID"]
    DD --> RR["5. Rerank<br/>CrossEncoder scoring"]
    RR --> OUT["Top-K Results"]
```

### Step 6.7 â€” CLI Tools

ðŸ“„ **Read in order:**
1. [rag.py](file:///Users/arkaj/Desktop/moodSwarm/tools/rag.py) â€” basic RAG CLI
2. [rag_tuning.py](file:///Users/arkaj/Desktop/moodSwarm/tools/rag_tuning.py) â€” latency + param sweep
3. [rag_eval.py](file:///Users/arkaj/Desktop/moodSwarm/tools/rag_eval.py) â€” Recall@K + MRR eval

### ðŸƒ Try It

```bash
# Run a RAG query in mock mode (no OpenAI calls)
python -m tools.rag --query "What is RAG?" --mock

# Run with real API
python -m tools.rag --query "What did Paul write about fine-tuning?"
```

### âœ… Self-Check
- [ ] What are the 5 stages of `ContextRetriever.search()`?
- [ ] Why use `mock=True`? What changes in each RAG step?
- [ ] What's the difference between bi-encoder (embedding) search and cross-encoder reranking?
- [ ] Why search 3 collections in parallel? How is deduplication done?
- [ ] What baseline metrics were achieved? (Recall@K, MRR, Latency)

---

## Module 7: Tooling & Observability

> **Goal:** Understand the supporting CLI tools and how to inspect the system.

### Step 7.1 â€” Main CLI Entry Point

ðŸ“„ **Read:** [run.py](file:///Users/arkaj/Desktop/moodSwarm/tools/run.py)

This is the unified CLI that runs all pipelines via flags.

### Step 7.2 â€” MongoDB Backup/Restore

ðŸ“„ **Read:** [data_warehouse.py](file:///Users/arkaj/Desktop/moodSwarm/tools/data_warehouse.py)

Export and import MongoDB data as JSON â€” essential for working offline without re-crawling.

### Step 7.3 â€” Qdrant Inspector

ðŸ“„ **Read:** [qdrant_inspect.py](file:///Users/arkaj/Desktop/moodSwarm/tools/qdrant_inspect.py)

CLI for listing collections, viewing stats, sampling points, and running semantic search.

### Step 7.4 â€” Chunk Analysis

ðŸ“„ **Read:** [chunk_analysis.py](file:///Users/arkaj/Desktop/moodSwarm/tools/chunk_analysis.py)

Validates that all chunks are within the embedding model's token limit.

### âœ… Self-Check
- [ ] How would you export all MongoDB data for backup?
- [ ] How do you verify chunks don't exceed the token limit?
- [ ] What CLI command runs a semantic search directly against Qdrant?

---

## ðŸŽ“ Recommended Reading Order (Summary)

| Order | Module | Files | Time |
|-------|--------|-------|------|
| 1 | Big Picture | `README.md` (top), `docker-compose.yml` | 20 min |
| 2 | Config & Infra | `settings.py`, `mongo.py`, `qdrant.py` | 30 min |
| 3 | Domain Models | `nosql.py` â†’ `vector.py` â†’ `documents.py` â†’ `cleaned_documents.py` â†’ `chunks.py` â†’ `embedded_chunks.py` â†’ `queries.py` | 45 min |
| 4 | ETL Pipeline | `dispatcher.py` â†’ crawlers â†’ `digital_data_etl.py` â†’ steps | 40 min |
| 5 | Feature Eng. | `dispatchers.py` â†’ cleaning â†’ chunking â†’ embedding â†’ pipeline steps | 50 min |
| 6 | RAG Retrieval | `base.py` â†’ `prompt_templates.py` â†’ `self_query.py` â†’ `query_expansion.py` â†’ `reranking.py` â†’ `retriever.py` | 60 min |
| 7 | Tooling | `run.py`, `data_warehouse.py`, `qdrant_inspect.py`, `chunk_analysis.py` | 20 min |

**Total: ~4.5 hours** for a complete walkthrough.

---

## ðŸ§© Design Patterns Cheat Sheet

| Pattern | Where | Why |
|---------|-------|-----|
| **Singleton** | DB connectors, Embedding model | One instance, reuse connection |
| **Factory** | Handler factories (cleaning, chunking, embedding) | Create the right handler per data type |
| **Strategy** | Individual handlers | Swap algorithms without changing caller |
| **Dispatcher** | `CrawlerDispatcher`, `CleaningDispatcher`, etc. | Route by URL/category |
| **Template Method** | `RAGStep` ABC | Enforce structure, allow customization |
| **ODM** | `NoSQLBaseDocument`, `VectorBaseDocument` | Map Python â†” database formats |

---

## What's Next? (Weeks 5â€“8 Preview)

| Week | What | Status |
|------|------|--------|
| 5 | Build instruction dataset + SFT fine-tuning | Pending |
| 6 | DPO preference alignment + eval | Pending |
| 7 | Inference optimization + deploy to AWS | Pending |
| 8 | MLOps monitoring + capstone | Pending |
