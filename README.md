# ğŸ§  MoodSwarm: LLM Twin & MLOps Platform

> Build an end-to-end AI system that mimics a specific persona's writing style and knowledge using the **FTI (Feature, Training, Inference) Architecture**.

![MoodSwarm Architecture](moodswarm.png)

### Progress

| Week | Phase | Status |
|------|-------|--------|
| 1 | Infrastructure & Environment Setup | Done |
| 2 | Digital Data ETL Pipeline | Done |
| 3 | RAG Feature Pipeline & Semantic Search | Done |
| 4 | RAG Retrieval & Inference | Next |
| 5 | Instruction Dataset & SFT Training | Pending |
| 6 | DPO Preference Alignment & Evaluation | Pending |
| 7 | Inference Optimization & Deployment | Pending |
| 8 | MLOps, Monitoring & Capstone | Pending |

---

## ğŸ—ï¸ System Architecture

### End-to-End Data Flow
```mermaid
graph LR
    subgraph "Data Sources"
        GH["GitHub Repos"]
        MD["Medium Articles"]
        SS["Substack / Custom"]
    end

    subgraph "ETL Pipeline (Week 2)"
        D[CrawlerDispatcher]
        GC[GithubCrawler]
        MC[MediumCrawler]
        CC[CustomArticleCrawler]
    end

    subgraph "Feature Pipeline (Week 3)"
        Clean["CleaningDispatcher"]
        Chunk["ChunkingDispatcher"]
        Embed["EmbeddingDispatcher"]
    end

    subgraph "Storage Layer"
        Mongo[(MongoDB)]
        Qdrant[(Qdrant)]
    end

    GH --> D
    MD --> D
    SS --> D
    D --> GC --> Mongo
    D --> MC --> Mongo
    D --> CC --> Mongo
    Mongo --> Clean --> Chunk --> Embed --> Qdrant
```

### Tech Stack
| Layer | Technology | Purpose |
|-------|------------|---------|
| Orchestration | **ZenML** | Pipeline DAGs, caching, artifact versioning |
| Raw Storage | **MongoDB** | Schemaless document store for crawled data |
| Vector Storage | **Qdrant** | ANN search with HNSW indexing (384-dim COSINE) |
| Embeddings | **all-MiniLM-L6-v2** | Sentence-level encoding (384 dimensions) |
| Language | **Python 3.11 + Poetry** | Reproducible dependency management |
| Containers | **Docker Compose** | Local MongoDB + Qdrant infrastructure |
| Architecture | **DDD** | Domain-Driven Design with layered separation |

---

## ğŸ“‚ Project Structure

```
moodSwarm/
â”œâ”€â”€ llm_engineering/                    # Core DDD Package
â”‚   â”œâ”€â”€ domain/                         # Data Models (Pure Python, no external deps)
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â”œâ”€â”€ nosql.py                #   MongoDB ODM (CRUD, UUID handling)
â”‚   â”‚   â”‚   â””â”€â”€ vector.py              #   Qdrant ODM (bulk_insert, search, auto-collection)
â”‚   â”‚   â”œâ”€â”€ documents.py               #   UserDocument, ArticleDocument, RepositoryDocument, PostDocument
â”‚   â”‚   â”œâ”€â”€ cleaned_documents.py       #   CleanedArticleDocument, CleanedPostDocument, CleanedRepositoryDocument
â”‚   â”‚   â”œâ”€â”€ chunks.py                  #   ArticleChunk, PostChunk, RepositoryChunk
â”‚   â”‚   â”œâ”€â”€ embedded_chunks.py         #   EmbeddedArticleChunk, EmbeddedPostChunk, EmbeddedRepositoryChunk
â”‚   â”‚   â”œâ”€â”€ queries.py                 #   Query, EmbeddedQuery (RAG retrieval query models)
â”‚   â”‚   â”œâ”€â”€ types.py                   #   DataCategory enum (posts, articles, repositories, prompts, datasets...)
â”‚   â”‚   â””â”€â”€ exceptions.py             #   LLMTwinException, ImproperlyConfigured
â”‚   â”‚
â”‚   â”œâ”€â”€ application/                    # Business Logic
â”‚   â”‚   â”œâ”€â”€ crawlers/                  #   GithubCrawler, MediumCrawler, CustomArticleCrawler, CrawlerDispatcher
â”‚   â”‚   â”œâ”€â”€ preprocessing/             #   Cleaning / Chunking / Embedding handlers + dispatchers + factories
â”‚   â”‚   â”‚   â”œâ”€â”€ dispatchers.py         #     CleaningDispatcher, ChunkingDispatcher, EmbeddingDispatcher
â”‚   â”‚   â”‚   â”œâ”€â”€ cleaning_data_handlers.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking_data_handlers.py
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_data_handlers.py
â”‚   â”‚   â”‚   â””â”€â”€ operations/            #     Low-level chunking + cleaning regex operations
â”‚   â”‚   â”œâ”€â”€ networks/                  #   EmbeddingModelSingleton, CrossEncoderModelSingleton
â”‚   â”‚   â””â”€â”€ utils/                     #   split_user_full_name, batch()
â”‚   â”‚
â”‚   â”œâ”€â”€ infrastructure/                 # External System Adapters
â”‚   â”‚   â””â”€â”€ db/
â”‚   â”‚       â”œâ”€â”€ mongo.py               #   MongoDatabaseConnector (Singleton)
â”‚   â”‚       â””â”€â”€ qdrant.py              #   QdrantDatabaseConnector (Singleton)
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                          # ML Model Code (future: SFT, DPO)
â”‚   â””â”€â”€ settings.py                     # Pydantic Settings (.env loader)
â”‚
â”œâ”€â”€ pipelines/                          # ZenML Pipeline Definitions
â”‚   â”œâ”€â”€ smoke_test.py                  #   Verify MongoDB + Qdrant connectivity
â”‚   â”œâ”€â”€ digital_data_etl.py            #   get_or_create_user â†’ crawl_links
â”‚   â””â”€â”€ feature_engineering.py         #   query_data_warehouse â†’ clean â†’ chunk_and_embed â†’ load_to_vector_db
â”‚
â”œâ”€â”€ steps/                              # ZenML Step Implementations
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ get_or_create_user.py      #   User lookup/creation + metadata logging
â”‚   â”‚   â””â”€â”€ crawl_links.py             #   Dispatcher-based crawling with retry + backoff
â”‚   â””â”€â”€ feature_engineering/
â”‚       â”œâ”€â”€ query_data_warehouse.py    #   Concurrent MongoDB fetch (ThreadPoolExecutor)
â”‚       â”œâ”€â”€ clean.py                   #   CleaningDispatcher per document
â”‚       â”œâ”€â”€ rag.py                     #   ChunkingDispatcher â†’ EmbeddingDispatcher (batch=10)
â”‚       â””â”€â”€ load_to_vector_db.py       #   group_by_class â†’ bulk_insert to Qdrant
â”‚
â”œâ”€â”€ configs/                            # Pipeline Parameter Files
â”‚   â”œâ”€â”€ digital_data_etl.yaml          #   User name + list of URLs to crawl
â”‚   â””â”€â”€ feature_engineering.yaml       #   Author names for feature extraction
â”‚
â”œâ”€â”€ tools/                              # CLI Utilities
â”‚   â”œâ”€â”€ run.py                         #   Main CLI (--run-smoke-test | --run-etl | --run-feature-engineering)
â”‚   â”œâ”€â”€ data_warehouse.py              #   MongoDB export/import (JSON backup/restore)
â”‚   â”œâ”€â”€ qdrant_inspect.py             #   Qdrant CLI (list-collections, stats, sample, semantic search)
â”‚   â”œâ”€â”€ chunk_analysis.py             #   Chunk validation (token distribution stats + PASS/FAIL limit check)
â”‚   â””â”€â”€ search_test.py               #   End-to-end semantic search across all embedded collections
â”‚
â”œâ”€â”€ interview/
â”‚   â””â”€â”€ INTERVIEW_QUESTIONS.md         #   41 interview Q&A derived from this codebase
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ data_save_flow.html            #   Interactive visualization of the data save flow
â”‚
â”œâ”€â”€ data/data_warehouse_raw_data/       #   Pre-crawled JSON data for offline import
â”œâ”€â”€ docker-compose.yml                  #   MongoDB + Qdrant containers
â””â”€â”€ pyproject.toml                      #   Poetry config + Poe tasks
```

---

## ğŸ“… Engineering Journal

### ğŸ”œ Week 4: RAG Retrieval & Inference *(Next)*
**Objective:** Advanced retrieval with query expansion, reranking, and baseline quality metrics.

- Self-query metadata extraction, query expansion, filtered vector search
- Reranking with CrossEncoder post-retrieval optimization
- Recall@K, MRR baselines and regression test set

### âœ… Week 3: RAG Feature Pipeline & Semantic Search
**Objective:** Transform raw text into searchable vectors in Qdrant, with end-to-end query capability.

**Feature Engineering Pipeline** (`feature_engineering`) with 4 ZenML steps:
1. `query_data_warehouse` â€” concurrent MongoDB fetch via `ThreadPoolExecutor`
2. `clean_documents` â€” regex normalization per data category
3. `chunk_and_embed` â€” type-specific splitting + SentenceTransformer encoding
4. `load_to_vector_db` â€” batched upsert into Qdrant (called twice: cleaned + embedded)

**Domain Models** â€” 11 classes across 4 transformation layers:
- **Cleaned:** `CleanedPostDocument`, `CleanedArticleDocument`, `CleanedRepositoryDocument`
- **Chunks:** `PostChunk`, `ArticleChunk`, `RepositoryChunk` (deterministic UUIDs via MD5)
- **Embedded:** `EmbeddedPostChunk`, `EmbeddedArticleChunk`, `EmbeddedRepositoryChunk` (384-dim vectors)
- **Queries:** `Query`, `EmbeddedQuery` â€” same embedding flow as chunks, enables RAG search

**Chunking Strategies (Two-Stage):**
- Posts: 250 tokens / 25 overlap â†’ token-capped at 256
- Articles: 1000-2000 chars sentence-aware â†’ token-capped at 256
- Repositories: 1500 tokens / 100 overlap â†’ token-capped at 256

**Query & Search Layer:**
- `Query.from_str()` factory + `EmbeddedQuery` with 384-dim embedding
- `QueryEmbeddingHandler` added to `EmbeddingDispatcher` â€” same bi-encoder, same vector space as chunks
- `tools/search_test.py` â€” end-to-end CLI: query string â†’ embed â†’ search all collections â†’ ranked results

**Validation & Tooling:**
- `tools/chunk_analysis.py` â€” token distribution analysis with PASS/FAIL limit checks
- `tools/qdrant_inspect.py` â€” collection listing, sampling, and semantic search CLI
- Idempotency verified: re-runs produce identical Qdrant counts
- All 26 chunks verified at or below 256 token limit

**Design Patterns:** Strategy (handlers), Factory (handler factories), Dispatcher (category routing), Singleton (embedding model), Open/Closed Principle (new QueryEmbeddingHandler without modifying existing handlers)

**Bugs Fixed:**
- `qdrant-client` API: `connection.search()` does not exist â†’ replaced with `connection.query_points()`
- Article chunks exceeded 256 token limit (380-443 tokens) â†’ added `SentenceTransformersTokenTextSplitter` as 2nd stage

**Final State:** `cleaned_articles` (3 points) + `embedded_articles` (26 points, 384-dim COSINE)

### âœ… Week 2: Digital Data ETL Pipeline
**Objective:** Automated data ingestion from the internet.

- **Pipeline:** `digital_data_etl` â€” `get_or_create_user` â†’ `crawl_links`
- **Crawlers:** GitHub (git clone + file walk), Medium (Selenium), Custom (LangChain)
- **Routing:** `CrawlerDispatcher` with regex-based URL matching + fallback
- **Resilience:** Exponential backoff retries (tenacity), deduplication via `.find(link=link)`
- **Tooling:** `tools/data_warehouse.py` for MongoDB JSON backup/restore
- **Result:** 3 articles crawled for Paul Iusztin (14K + 8K + 7K chars), zero duplicates on re-run

### âœ… Week 1: Infrastructure Foundation
**Objective:** Reproducible MLOps environment.

- Docker Compose for MongoDB (27017) + Qdrant (6333)
- ZenML local stack initialization
- Pydantic Settings for `.env`-based configuration
- Smoke test pipeline for connectivity validation

---

## ğŸ” Pipeline Deep Dives

### ETL Pipeline Sequence
```mermaid
sequenceDiagram
    participant CLI as tools/run.py
    participant ZML as ZenML
    participant S1 as get_or_create_user
    participant S2 as crawl_links
    participant DB as MongoDB

    CLI->>ZML: --run-etl
    ZML->>S1: Execute("Paul Iusztin")
    S1->>DB: UserDocument.get_or_create()
    DB-->>S1: UserDocument(id=uuid)
    ZML->>S2: Execute(user, [url1, url2, url3])
    loop Each URL
        S2->>S2: CrawlerDispatcher â†’ select crawler
        S2->>S2: crawler.extract() with retry
        S2->>DB: ArticleDocument.save() (deduplicated)
    end
    S2-->>ZML: âœ… 3/3 crawled
```

### Feature Engineering Pipeline
```mermaid
graph TD
    A[MongoDB Raw Documents] --> B[query_data_warehouse]
    B --> |ThreadPoolExecutor| C[clean_documents]
    C --> |CleaningDispatcher| D["CleanedDocuments"]
    D --> E1[load_to_vector_db]
    E1 --> F1["Qdrant: cleaned_* collections"]
    D --> G[chunk_and_embed]
    G --> |ChunkingDispatcher| H[Chunks]
    H --> |"EmbeddingDispatcher (batch=10)"| I["EmbeddedChunks (384-dim)"]
    I --> E2[load_to_vector_db]
    E2 --> F2["Qdrant: embedded_* collections"]
```

---

## ğŸ”€ End-to-End Data Flow: How Data is Saved & Transformed

> ğŸ“„ **Interactive version:** Open [`docs/data_save_flow.html`](docs/data_save_flow.html) in a browser for a styled, step-by-step visualization.

### Complete Data Lifecycle
```mermaid
sequenceDiagram
    participant URL as ğŸŒ URL
    participant Crawler as CrawlerDispatcher
    participant PyObj as ğŸ Python Object
    participant ODM as NoSQLBaseDocument
    participant Mongo as ğŸƒ MongoDB
    participant FE as Feature Pipeline
    participant Clean as CleaningDispatcher
    participant Chunk as ChunkingDispatcher
    participant Embed as EmbeddingDispatcher
    participant VecODM as VectorBaseDocument
    participant Qdrant as ğŸ”· Qdrant

    Note over URL,Mongo: WEEK 2 â€” ETL Pipeline
    URL->>Crawler: URL string
    Crawler->>Crawler: Regex match â†’ pick crawler
    Crawler->>PyObj: ArticleDocument(id=UUID, content=..., link=...)
    PyObj->>ODM: .save()
    ODM->>ODM: to_mongo(): idâ†’_id, UUIDâ†’string
    ODM->>Mongo: insert_one(dict)

    Note over Mongo,Qdrant: WEEK 3 â€” Feature Pipeline
    Mongo->>ODM: find() returns raw dict
    ODM->>ODM: from_mongo(): _idâ†’id, stringâ†’UUID
    ODM->>FE: List of ArticleDocument objects
    FE->>Clean: Per document
    Clean->>Clean: Regex normalize text
    Clean->>Chunk: CleanedArticleDocument
    Chunk->>Chunk: Split into chunks (sentence-aware)
    Chunk->>Embed: List of ArticleChunks
    Embed->>Embed: MiniLM encode â†’ 384-dim vectors
    Embed->>VecODM: EmbeddedArticleChunk(embedding=[...])
    VecODM->>VecODM: to_point(): extract vector from payload
    VecODM->>Qdrant: bulk_insert(PointStruct)
```

### ODM Transformation: How Python â†” Database Bridging Works

The project uses **two custom ODM layers** that transparently handle format conversion:

#### MongoDB ODM (`NoSQLBaseDocument`)
| Stage | `id` field | Key name | Type |
|-------|-----------|----------|------|
| **Python creation** | `UUID('a1b2c3d4-...')` | `id` | Python UUID object |
| **`to_mongo()`** | `'a1b2c3d4-...'` | `_id` | Plain string â† renamed |
| **MongoDB disk** | `'a1b2c3d4-...'` | `_id` | BSON string |
| **`from_mongo()`** | `'a1b2c3d4-...'` â†’ `UUID(...)` | `id` | Pydantic coerces back |

```python
# SAVE: Python â†’ MongoDB
def to_mongo(self) -> dict:
    data = self.model_dump()
    data['_id'] = str(data.pop('id'))   # UUID object â†’ string, 'id' â†’ '_id'
    return data

# LOAD: MongoDB â†’ Python
def from_mongo(cls, data: dict):
    if '_id' in data:
        data['id'] = data.pop('_id')    # '_id' â†’ 'id'
    return cls(**data)                   # Pydantic coerces string â†’ UUID
```

#### Qdrant ODM (`VectorBaseDocument`)
| Stage | Key transformation | Purpose |
|-------|-------------------|---------|
| **`to_point()`** | Extract `embedding` from payload, convert `numpy` â†’ `list` | Qdrant needs vectors separate from payload |
| **`from_record()`** | Merge `record.id` + `record.payload`, conditionally set `embedding` | Reconstruct full Python object from Qdrant record |

```python
# SAVE: Python â†’ Qdrant
def to_point(self) -> PointStruct:
    data = self.model_dump()
    vector = data.pop("embedding", [])
    _id = str(data.pop("id"))
    return PointStruct(id=_id, vector=vector, payload=data)

# LOAD: Qdrant â†’ Python
def from_record(cls, record) -> "VectorBaseDocument":
    payload = record.payload or {}
    payload["id"] = record.id
    if cls._has_class_attribute("embedding"):
        payload["embedding"] = record.vector
    return cls(**payload)
```

### Data Object Shapes at Each Stage

```
URL: "https://medium.com/@user/my-post"
                    â”‚
                    â–¼
â”Œâ”€ ArticleDocument (Python) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  id:        UUID('f9e8d7c6-...')                     â”‚â”€â”€â”€â”€ .save() â†’ to_mongo()
â”‚  platform:  "medium"                                 â”‚
â”‚  link:      "https://medium.com/@user/my-post"       â”‚
â”‚  content:   {"title": "...", "text": "..."}          â”‚
â”‚  author_id: UUID('a1b2c3d4-...')                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€ MongoDB Document (BSON on disk) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  _id:       "f9e8d7c6-..."          â† UUID â†’ string  â”‚
â”‚  platform:  "medium"                                  â”‚
â”‚  link:      "https://medium.com/..."                  â”‚
â”‚  content:   {"title": "...", "text": "..."}           â”‚
â”‚  author_id: "a1b2c3d4-..."          â† UUID â†’ string  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼  CleaningDispatcher
â”Œâ”€ CleanedArticleDocument (Qdrant payload-only) â”€â”€â”€â”€â”€â”€â”€â”
â”‚  id:        UUID(MD5(content))       â† deterministic  â”‚
â”‚  content:   "cleaned plain text..."  â† regex cleaned  â”‚
â”‚  platform:  "medium"                                  â”‚
â”‚  author_id: UUID('a1b2c3d4-...')                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼  ChunkingDispatcher (1000-2000 chars, sentence-aware)
â”Œâ”€ ArticleChunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  id:        UUID(MD5(chunk_content)) â† per-chunk ID   â”‚
â”‚  content:   "one paragraph chunk..."                  â”‚
â”‚  chunk_id:  0                                         â”‚
â”‚  metadata:  {chunk_size: 500, overlap: 50}            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼  EmbeddingDispatcher (MiniLM, batch=10)
â”Œâ”€ EmbeddedArticleChunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  id:        UUID(MD5(chunk_content))                  â”‚
â”‚  content:   "one paragraph chunk..."                  â”‚
â”‚  embedding: [0.023, -0.156, ..., 0.089]  â† 384 floatsâ”‚â”€â”€â”€â”€ .to_point()
â”‚  metadata:  {model: "all-MiniLM-L6-v2", dim: 384}   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€ Qdrant PointStruct â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  id:      "f9e8d7c6-..."                              â”‚
â”‚  vector:  [0.023, -0.156, ..., 0.089]  â† separate    â”‚
â”‚  payload: {content: "...", platform: "medium", ...}   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ How to Run

### 1. Start Infrastructure
```bash
docker-compose up -d
```

### 2. Run Pipelines
```bash
# Connectivity check
poetry run python -m tools.run --run-smoke-test

# Crawl data from the internet â†’ MongoDB
poetry run python -m tools.run --run-etl --no-cache

# Clean â†’ Chunk â†’ Embed â†’ Qdrant
poetry run python -m tools.run --run-feature-engineering --no-cache
```

### 3. Inspect Vector Store
```bash
# List all Qdrant collections
poetry run python -m tools.qdrant_inspect list-collections

# View sample points
poetry run python -m tools.qdrant_inspect sample embedded_articles --limit 3

# Semantic search (via qdrant_inspect)
poetry run python -m tools.qdrant_inspect search embedded_articles --query "machine learning deployment"
```

### 3b. End-to-End Semantic Search
```bash
# Search across ALL embedded collections at once
poetry run python -m tools.search_test --query "machine learning deployment"

# Custom number of results per collection
poetry run python -m tools.search_test --query "data pipelines" --k 5
```

### 4. Validate Chunk Quality
```bash
# Analyze token distributions across all embedded_* collections
poetry run python -m tools.chunk_analysis

# Check a specific collection
poetry run python -m tools.chunk_analysis --collection embedded_articles
```

### 5. Data Backup/Restore
```bash
# Export MongoDB â†’ JSON
poetry run python -m tools.data_warehouse --export-raw-data

# Import JSON â†’ MongoDB
poetry run python -m tools.data_warehouse --import-raw-data
```

### 6. Monitoring
```bash
poetry run zenml login --local
```

---

## ğŸ“ Interview Preparation

A comprehensive set of **45 interview questions** derived directly from this codebase is available at [`interview/INTERVIEW_QUESTIONS.md`](interview/INTERVIEW_QUESTIONS.md). Topics covered:
- System Architecture & FTI Design
- Data Engineering & ETL Patterns
- Feature Pipeline (Clean â†’ Chunk â†’ Embed)
- Domain Modeling & ODM Patterns
- Embeddings & NLP Theory
- Vector Databases & Similarity Search
- Software Design Patterns (Strategy, Factory, Dispatcher, Singleton)
- MLOps & Pipeline Orchestration
- RAG Retrieval & Query Embedding
- Model Training (QLoRA, SFT, DPO)
- Mathematical Foundations
- Testing & Validation (chunk quality gates)
