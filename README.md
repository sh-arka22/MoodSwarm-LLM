# ğŸ§  MoodSwarm: LLM Twin & MLOps Platform

> Build an end-to-end AI system that mimics a specific persona's writing style and knowledge using the **FTI (Feature, Training, Inference) Architecture**.

![MoodSwarm Architecture](moodswarm.png)

---

## ğŸ—ï¸ System Architecture

### End-to-End Data Flow
```mermaid
graph LR
    subgraph "Data Sources"
        GH["GitHub Repos"]
        MD["Medium Articles"]
        SS["Substack / Custom"]
    end

    subgraph "ETL Pipeline (Week 2)"
        D[CrawlerDispatcher]
        GC[GithubCrawler]
        MC[MediumCrawler]
        CC[CustomArticleCrawler]
    end

    subgraph "Feature Pipeline (Week 3)"
        Clean["CleaningDispatcher"]
        Chunk["ChunkingDispatcher"]
        Embed["EmbeddingDispatcher"]
    end

    subgraph "Storage Layer"
        Mongo[(MongoDB)]
        Qdrant[(Qdrant)]
    end

    GH --> D
    MD --> D
    SS --> D
    D --> GC --> Mongo
    D --> MC --> Mongo
    D --> CC --> Mongo
    Mongo --> Clean --> Chunk --> Embed --> Qdrant
```

### Tech Stack
| Layer | Technology | Purpose |
|-------|------------|---------|
| Orchestration | **ZenML** | Pipeline DAGs, caching, artifact versioning |
| Raw Storage | **MongoDB** | Schemaless document store for crawled data |
| Vector Storage | **Qdrant** | ANN search with HNSW indexing (384-dim COSINE) |
| Embeddings | **all-MiniLM-L6-v2** | Sentence-level encoding (384 dimensions) |
| Language | **Python 3.11 + Poetry** | Reproducible dependency management |
| Containers | **Docker Compose** | Local MongoDB + Qdrant infrastructure |
| Architecture | **DDD** | Domain-Driven Design with layered separation |

---

## ğŸ“‚ Project Structure

```
moodSwarm/
â”œâ”€â”€ llm_engineering/                    # Core DDD Package
â”‚   â”œâ”€â”€ domain/                         # Data Models (Pure Python, no external deps)
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â”œâ”€â”€ nosql.py                #   MongoDB ODM (CRUD, UUID handling)
â”‚   â”‚   â”‚   â””â”€â”€ vector.py              #   Qdrant ODM (bulk_insert, search, auto-collection)
â”‚   â”‚   â”œâ”€â”€ documents.py               #   UserDocument, ArticleDocument, RepositoryDocument, PostDocument
â”‚   â”‚   â”œâ”€â”€ cleaned_documents.py       #   CleanedArticleDocument, CleanedPostDocument, CleanedRepositoryDocument
â”‚   â”‚   â”œâ”€â”€ chunks.py                  #   ArticleChunk, PostChunk, RepositoryChunk
â”‚   â”‚   â”œâ”€â”€ embedded_chunks.py         #   EmbeddedArticleChunk, EmbeddedPostChunk, EmbeddedRepositoryChunk
â”‚   â”‚   â”œâ”€â”€ types.py                   #   DataCategory enum (posts, articles, repositories, prompts, datasets...)
â”‚   â”‚   â””â”€â”€ exceptions.py             #   LLMTwinException, ImproperlyConfigured
â”‚   â”‚
â”‚   â”œâ”€â”€ application/                    # Business Logic
â”‚   â”‚   â”œâ”€â”€ crawlers/                  #   GithubCrawler, MediumCrawler, CustomArticleCrawler, CrawlerDispatcher
â”‚   â”‚   â”œâ”€â”€ preprocessing/             #   Cleaning / Chunking / Embedding handlers + dispatchers + factories
â”‚   â”‚   â”‚   â”œâ”€â”€ dispatchers.py         #     CleaningDispatcher, ChunkingDispatcher, EmbeddingDispatcher
â”‚   â”‚   â”‚   â”œâ”€â”€ cleaning_data_handlers.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking_data_handlers.py
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_data_handlers.py
â”‚   â”‚   â”‚   â””â”€â”€ operations/            #     Low-level chunking + cleaning regex operations
â”‚   â”‚   â”œâ”€â”€ networks/                  #   EmbeddingModelSingleton, CrossEncoderModelSingleton
â”‚   â”‚   â””â”€â”€ utils/                     #   split_user_full_name, batch()
â”‚   â”‚
â”‚   â”œâ”€â”€ infrastructure/                 # External System Adapters
â”‚   â”‚   â””â”€â”€ db/
â”‚   â”‚       â”œâ”€â”€ mongo.py               #   MongoDatabaseConnector (Singleton)
â”‚   â”‚       â””â”€â”€ qdrant.py              #   QdrantDatabaseConnector (Singleton)
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                          # ML Model Code (future: SFT, DPO)
â”‚   â””â”€â”€ settings.py                     # Pydantic Settings (.env loader)
â”‚
â”œâ”€â”€ pipelines/                          # ZenML Pipeline Definitions
â”‚   â”œâ”€â”€ smoke_test.py                  #   Verify MongoDB + Qdrant connectivity
â”‚   â”œâ”€â”€ digital_data_etl.py            #   get_or_create_user â†’ crawl_links
â”‚   â””â”€â”€ feature_engineering.py         #   query_data_warehouse â†’ clean â†’ chunk_and_embed â†’ load_to_vector_db
â”‚
â”œâ”€â”€ steps/                              # ZenML Step Implementations
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ get_or_create_user.py      #   User lookup/creation + metadata logging
â”‚   â”‚   â””â”€â”€ crawl_links.py             #   Dispatcher-based crawling with retry + backoff
â”‚   â””â”€â”€ feature_engineering/
â”‚       â”œâ”€â”€ query_data_warehouse.py    #   Concurrent MongoDB fetch (ThreadPoolExecutor)
â”‚       â”œâ”€â”€ clean.py                   #   CleaningDispatcher per document
â”‚       â”œâ”€â”€ rag.py                     #   ChunkingDispatcher â†’ EmbeddingDispatcher (batch=10)
â”‚       â””â”€â”€ load_to_vector_db.py       #   group_by_class â†’ bulk_insert to Qdrant
â”‚
â”œâ”€â”€ configs/                            # Pipeline Parameter Files
â”‚   â”œâ”€â”€ digital_data_etl.yaml          #   User name + list of URLs to crawl
â”‚   â””â”€â”€ feature_engineering.yaml       #   Author names for feature extraction
â”‚
â”œâ”€â”€ tools/                              # CLI Utilities
â”‚   â”œâ”€â”€ run.py                         #   Main CLI (--run-smoke-test | --run-etl | --run-feature-engineering)
â”‚   â”œâ”€â”€ data_warehouse.py              #   MongoDB export/import (JSON backup/restore)
â”‚   â”œâ”€â”€ qdrant_inspect.py             #   Qdrant CLI (list-collections, stats, sample, semantic search)
â”‚   â””â”€â”€ chunk_analysis.py             #   Chunk validation (token distribution stats + PASS/FAIL limit check)
â”‚
â”œâ”€â”€ interview/
â”‚   â””â”€â”€ INTERVIEW_QUESTIONS.md         #   41 interview Q&A derived from this codebase
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ data_save_flow.html            #   Interactive visualization of the data save flow
â”‚
â”œâ”€â”€ data/data_warehouse_raw_data/       #   Pre-crawled JSON data for offline import
â”œâ”€â”€ docker-compose.yml                  #   MongoDB + Qdrant containers
â””â”€â”€ pyproject.toml                      #   Poetry config + Poe tasks
```

---

## ğŸ“… Engineering Journal

### ğŸ”„ Week 3: RAG Feature Pipeline *(In Progress)*
**Objective:** Transform raw text â†’ searchable vectors in Qdrant.

- **Pipeline:** `feature_engineering` with 4 ZenML steps:
  1. `query_data_warehouse` â€” concurrent MongoDB fetch via `ThreadPoolExecutor`
  2. `clean_documents` â€” regex normalization per data category
  3. `chunk_and_embed` â€” type-specific splitting + SentenceTransformer encoding
  4. `load_to_vector_db` â€” batched upsert into Qdrant (called twice: cleaned + embedded)
- **Domain Models:** 9 new classes across 3 transformation layers:
  - **Cleaned:** `CleanedPostDocument`, `CleanedArticleDocument`, `CleanedRepositoryDocument`
  - **Chunks:** `PostChunk`, `ArticleChunk`, `RepositoryChunk` (deterministic UUIDs)
  - **Embedded:** `EmbeddedPostChunk`, `EmbeddedArticleChunk`, `EmbeddedRepositoryChunk` (384-dim vectors)
- **Chunking Strategies (Two-Stage):**
  - Posts: 250 tokens / 25 overlap â†’ token-capped at 256
  - Articles: 1000â€“2000 chars sentence-aware â†’ token-capped at 256
  - Repositories: 1500 tokens / 100 overlap â†’ token-capped at 256
- **Validation:** `tools/chunk_analysis.py` scans all embedded collections, reports min/max/avg token counts per collection, and PASS/FAIL checks against the 256-token model limit
- **Tooling:** Built `tools/qdrant_inspect.py` CLI for listing collections, sampling points, and running semantic searches
- **Design Patterns:** Strategy (handlers), Factory (handler factories), Dispatcher (category routing), Singleton (embedding model)

### âœ… Week 2: Digital Data ETL Pipeline
**Objective:** Automated data ingestion from the internet.

- **Pipeline:** `digital_data_etl` â€” `get_or_create_user` â†’ `crawl_links`
- **Crawlers:** GitHub (git clone + file walk), Medium (Selenium), Custom (LangChain)
- **Routing:** `CrawlerDispatcher` with regex-based URL matching + fallback
- **Resilience:** Exponential backoff retries (tenacity), deduplication via `.find(link=link)`
- **Tooling:** `tools/data_warehouse.py` for MongoDB JSON backup/restore

### âœ… Week 1: Infrastructure Foundation
**Objective:** Reproducible MLOps environment.

- Docker Compose for MongoDB (27017) + Qdrant (6333)
- ZenML local stack initialization
- Pydantic Settings for `.env`-based configuration
- Smoke test pipeline for connectivity validation

---

## ğŸ” Pipeline Deep Dives

### ETL Pipeline Sequence
```mermaid
sequenceDiagram
    participant CLI as tools/run.py
    participant ZML as ZenML
    participant S1 as get_or_create_user
    participant S2 as crawl_links
    participant DB as MongoDB

    CLI->>ZML: --run-etl
    ZML->>S1: Execute("Paul Iusztin")
    S1->>DB: UserDocument.get_or_create()
    DB-->>S1: UserDocument(id=uuid)
    ZML->>S2: Execute(user, [url1, url2, url3])
    loop Each URL
        S2->>S2: CrawlerDispatcher â†’ select crawler
        S2->>S2: crawler.extract() with retry
        S2->>DB: ArticleDocument.save() (deduplicated)
    end
    S2-->>ZML: âœ… 3/3 crawled
```

### Feature Engineering Pipeline
```mermaid
graph TD
    A[MongoDB Raw Documents] --> B[query_data_warehouse]
    B --> |ThreadPoolExecutor| C[clean_documents]
    C --> |CleaningDispatcher| D["CleanedDocuments"]
    D --> E1[load_to_vector_db]
    E1 --> F1["Qdrant: cleaned_* collections"]
    D --> G[chunk_and_embed]
    G --> |ChunkingDispatcher| H[Chunks]
    H --> |"EmbeddingDispatcher (batch=10)"| I["EmbeddedChunks (384-dim)"]
    I --> E2[load_to_vector_db]
    E2 --> F2["Qdrant: embedded_* collections"]
```

---

## ğŸ”€ End-to-End Data Flow: How Data is Saved & Transformed

> ğŸ“„ **Interactive version:** Open [`docs/data_save_flow.html`](docs/data_save_flow.html) in a browser for a styled, step-by-step visualization.

### Complete Data Lifecycle
```mermaid
sequenceDiagram
    participant URL as ğŸŒ URL
    participant Crawler as CrawlerDispatcher
    participant PyObj as ğŸ Python Object
    participant ODM as NoSQLBaseDocument
    participant Mongo as ğŸƒ MongoDB
    participant FE as Feature Pipeline
    participant Clean as CleaningDispatcher
    participant Chunk as ChunkingDispatcher
    participant Embed as EmbeddingDispatcher
    participant VecODM as VectorBaseDocument
    participant Qdrant as ğŸ”· Qdrant

    Note over URL,Mongo: WEEK 2 â€” ETL Pipeline
    URL->>Crawler: URL string
    Crawler->>Crawler: Regex match â†’ pick crawler
    Crawler->>PyObj: ArticleDocument(id=UUID, content=..., link=...)
    PyObj->>ODM: .save()
    ODM->>ODM: to_mongo(): idâ†’_id, UUIDâ†’string
    ODM->>Mongo: insert_one(dict)

    Note over Mongo,Qdrant: WEEK 3 â€” Feature Pipeline
    Mongo->>ODM: find() returns raw dict
    ODM->>ODM: from_mongo(): _idâ†’id, stringâ†’UUID
    ODM->>FE: List of ArticleDocument objects
    FE->>Clean: Per document
    Clean->>Clean: Regex normalize text
    Clean->>Chunk: CleanedArticleDocument
    Chunk->>Chunk: Split into chunks (sentence-aware)
    Chunk->>Embed: List of ArticleChunks
    Embed->>Embed: MiniLM encode â†’ 384-dim vectors
    Embed->>VecODM: EmbeddedArticleChunk(embedding=[...])
    VecODM->>VecODM: to_point(): extract vector from payload
    VecODM->>Qdrant: bulk_insert(PointStruct)
```

### ODM Transformation: How Python â†” Database Bridging Works

The project uses **two custom ODM layers** that transparently handle format conversion:

#### MongoDB ODM (`NoSQLBaseDocument`)
| Stage | `id` field | Key name | Type |
|-------|-----------|----------|------|
| **Python creation** | `UUID('a1b2c3d4-...')` | `id` | Python UUID object |
| **`to_mongo()`** | `'a1b2c3d4-...'` | `_id` | Plain string â† renamed |
| **MongoDB disk** | `'a1b2c3d4-...'` | `_id` | BSON string |
| **`from_mongo()`** | `'a1b2c3d4-...'` â†’ `UUID(...)` | `id` | Pydantic coerces back |

```python
# SAVE: Python â†’ MongoDB
def to_mongo(self) -> dict:
    data = self.model_dump()
    data['_id'] = str(data.pop('id'))   # UUID object â†’ string, 'id' â†’ '_id'
    return data

# LOAD: MongoDB â†’ Python
def from_mongo(cls, data: dict):
    if '_id' in data:
        data['id'] = data.pop('_id')    # '_id' â†’ 'id'
    return cls(**data)                   # Pydantic coerces string â†’ UUID
```

#### Qdrant ODM (`VectorBaseDocument`)
| Stage | Key transformation | Purpose |
|-------|-------------------|---------|
| **`to_point()`** | Extract `embedding` from payload, convert `numpy` â†’ `list` | Qdrant needs vectors separate from payload |
| **`from_record()`** | Merge `record.id` + `record.payload`, conditionally set `embedding` | Reconstruct full Python object from Qdrant record |

```python
# SAVE: Python â†’ Qdrant
def to_point(self) -> PointStruct:
    data = self.model_dump()
    vector = data.pop("embedding", [])
    _id = str(data.pop("id"))
    return PointStruct(id=_id, vector=vector, payload=data)

# LOAD: Qdrant â†’ Python
def from_record(cls, record) -> "VectorBaseDocument":
    payload = record.payload or {}
    payload["id"] = record.id
    if cls._has_class_attribute("embedding"):
        payload["embedding"] = record.vector
    return cls(**payload)
```

### Data Object Shapes at Each Stage

```
URL: "https://medium.com/@user/my-post"
                    â”‚
                    â–¼
â”Œâ”€ ArticleDocument (Python) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  id:        UUID('f9e8d7c6-...')                     â”‚â”€â”€â”€â”€ .save() â†’ to_mongo()
â”‚  platform:  "medium"                                 â”‚
â”‚  link:      "https://medium.com/@user/my-post"       â”‚
â”‚  content:   {"title": "...", "text": "..."}          â”‚
â”‚  author_id: UUID('a1b2c3d4-...')                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€ MongoDB Document (BSON on disk) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  _id:       "f9e8d7c6-..."          â† UUID â†’ string  â”‚
â”‚  platform:  "medium"                                  â”‚
â”‚  link:      "https://medium.com/..."                  â”‚
â”‚  content:   {"title": "...", "text": "..."}           â”‚
â”‚  author_id: "a1b2c3d4-..."          â† UUID â†’ string  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼  CleaningDispatcher
â”Œâ”€ CleanedArticleDocument (Qdrant payload-only) â”€â”€â”€â”€â”€â”€â”€â”
â”‚  id:        UUID(MD5(content))       â† deterministic  â”‚
â”‚  content:   "cleaned plain text..."  â† regex cleaned  â”‚
â”‚  platform:  "medium"                                  â”‚
â”‚  author_id: UUID('a1b2c3d4-...')                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼  ChunkingDispatcher (1000-2000 chars, sentence-aware)
â”Œâ”€ ArticleChunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  id:        UUID(MD5(chunk_content)) â† per-chunk ID   â”‚
â”‚  content:   "one paragraph chunk..."                  â”‚
â”‚  chunk_id:  0                                         â”‚
â”‚  metadata:  {chunk_size: 500, overlap: 50}            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼  EmbeddingDispatcher (MiniLM, batch=10)
â”Œâ”€ EmbeddedArticleChunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  id:        UUID(MD5(chunk_content))                  â”‚
â”‚  content:   "one paragraph chunk..."                  â”‚
â”‚  embedding: [0.023, -0.156, ..., 0.089]  â† 384 floatsâ”‚â”€â”€â”€â”€ .to_point()
â”‚  metadata:  {model: "all-MiniLM-L6-v2", dim: 384}   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€ Qdrant PointStruct â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  id:      "f9e8d7c6-..."                              â”‚
â”‚  vector:  [0.023, -0.156, ..., 0.089]  â† separate    â”‚
â”‚  payload: {content: "...", platform: "medium", ...}   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ How to Run

### 1. Start Infrastructure
```bash
docker-compose up -d
```

### 2. Run Pipelines
```bash
# Connectivity check
poetry run python -m tools.run --run-smoke-test

# Crawl data from the internet â†’ MongoDB
poetry run python -m tools.run --run-etl --no-cache

# Clean â†’ Chunk â†’ Embed â†’ Qdrant
poetry run python -m tools.run --run-feature-engineering --no-cache
```

### 3. Inspect Vector Store
```bash
# List all Qdrant collections
poetry run python tools/qdrant_inspect.py list-collections

# View sample points
poetry run python tools/qdrant_inspect.py sample embedded_articles --limit 3

# Semantic search
poetry run python tools/qdrant_inspect.py search embedded_articles --query "machine learning deployment"
```

### 4. Validate Chunk Quality
```bash
# Analyze token distributions across all embedded_* collections
poetry run python tools/chunk_analysis.py

# Check a specific collection
poetry run python tools/chunk_analysis.py --collection embedded_articles
```

### 5. Data Backup/Restore
```bash
# Export MongoDB â†’ JSON
poetry run python tools/data_warehouse.py --export-raw-data

# Import JSON â†’ MongoDB
poetry run python tools/data_warehouse.py --import-raw-data
```

### 6. Monitoring
```bash
poetry run zenml login --local
```

---

## ğŸ“ Interview Preparation

A comprehensive set of **41 interview questions** derived directly from this codebase is available at [`interview/INTERVIEW_QUESTIONS.md`](interview/INTERVIEW_QUESTIONS.md). Topics covered:
- System Architecture & FTI Design
- Data Engineering & ETL Patterns
- Feature Pipeline (Clean â†’ Chunk â†’ Embed)
- Domain Modeling & ODM Patterns
- Embeddings & NLP Theory
- Vector Databases & Similarity Search
- Software Design Patterns (Strategy, Factory, Dispatcher, Singleton)
- MLOps & Pipeline Orchestration
- Model Training (QLoRA, SFT, DPO)
- Mathematical Foundations
- Testing & Validation (chunk quality gates)
